---
# SA-3 (Stakeholder Modeler) CYCLE_1 (BROAD) Review of PRD-0005
# Generated: 2026-01-26
# Review Protocol: Multi-mode epistemic review with North Star alignment

review_meta:
  agent_id: SA-3
  agent_role: Stakeholder Modeler
  prd_id: PRD-0005
  cycle: 1
  cycle_type: BROAD
  review_timestamp: "2026-01-26T00:00:00Z"
  modes_applied:
    - 56  # Theory-of-mind / mental-state reasoning (Strategic)
    - 36  # Assurance-case / safety-case reasoning (Inconsistency)
    - 46  # Multi-criteria decision analysis (Practical)
    - 76  # Calibration and epistemic humility (Meta)
    - 80  # Debiasing / epistemic hygiene (Meta)

findings:
  # MODE 56: Theory-of-Mind / Mental-State Reasoning
  # Focus: Model stakeholder perspectives and needs

  - finding_id: FND-SA3-0001
    source_agent: SA-3
    source_cycle: 1
    source_mode: 56
    category: SPEC_DEFECT
    subcategory: STAKEHOLDER_MODEL_INCOMPLETE
    severity: MAJOR
    location:
      file: documents/prds/PRD-0005/01_customer.yaml
      section: segments.CUST-0002
    description: |
      The "Spec compiler holon (Idea Compiler)" customer segment (CUST-0002) lacks
      explicit failure recovery perspectives. The segment describes crash-only and
      resumable characteristics but does not model the holon's mental state during
      partial compilation failure scenarios.

      From a theory-of-mind perspective, when an agent compiler holon encounters an
      ambiguous requirement-to-component mapping, what are its beliefs about:
      - What the human operator expects to happen next?
      - Whether to fail-fast or attempt best-effort mapping with uncertainty flags?
      - How to communicate confidence levels in its mapping decisions?

      The PRD states the holon "must incorporate explicit repository truth" but does
      not model how the holon should reason about incomplete or conflicting repository
      information (e.g., AGENTS.md invariants contradicting actual code structure).
    remediation: |
      Add to CUST-0002.primary_jobs_to_be_done:
      - "Signal confidence levels and request human adjudication when CCP contains
         conflicting signals (e.g., AGENTS.md claims component X is stable, but git
         history shows high churn)."
      - "Model operator intent from PRD context to disambiguate between 'extend
         existing' vs 'create new' when both are plausible."

      Add to CUST-0002.constraints:
      - "The holon MUST communicate its uncertainty level (high/medium/low confidence)
         per Impact Map entry, not just binary pass/fail."

  - finding_id: FND-SA3-0002
    source_agent: SA-3
    source_cycle: 1
    source_mode: 56
    category: SPEC_DEFECT
    subcategory: COMMUNICATION_STRATEGY_UNDEFINED
    severity: MAJOR
    location:
      file: documents/prds/PRD-0005/requirements/REQ-0003.yaml
      section: acceptance_criteria
    description: |
      REQ-0003 (Requirement-to-component impact mapping) specifies that "Requirements
      that cannot be mapped are explicitly classified as net-new substrate and require
      an adjudication record before RFC emission." However, the acceptance criteria
      do not specify HOW this adjudication request should be communicated to maximize
      human operator comprehension.

      Theory-of-mind analysis reveals a perspective gap: The PRD assumes the operator
      will understand why a mapping failed, but different operators may have different
      mental models:
      - A senior architect expects to see alternative extension points considered.
      - A PM expects to see customer impact analysis.
      - A security reviewer expects to see threat surface implications.

      The current design produces a single adjudication record format, which may not
      align with diverse stakeholder mental models.
    remediation: |
      Add to REQ-0003.acceptance_criteria:
      - "Adjudication records MUST include: (a) alternative extension points considered
         with fit scores, (b) rationale for why each was rejected, (c) estimated
         complexity/risk of net-new substrate, and (d) stakeholder-specific impacts
         (security, operations, maintenance)."

      Consider adding REQ-0013 for "Adjudication Record Format" with audience-aware
      presentation (architect view, PM view, security view).

  - finding_id: FND-SA3-0003
    source_agent: SA-3
    source_cycle: 1
    source_mode: 56
    category: EVIDENCE_DEFECT
    subcategory: PERSPECTIVE_COVERAGE_GAP
    severity: MINOR
    location:
      file: documents/prds/PRD-0005/evidence_artifacts/EVID-0010.yaml
      section: verification.commands
    description: |
      EVID-0010 (Skill sync and instruction drift verification) verifies that "Skill
      outputs must not instruct agents to bypass the compiler pipeline," but it does
      not verify that skill instructions are comprehensible from multiple agent
      capability levels (novice holon vs. expert holon).

      Theory-of-mind concern: A highly capable agent may correctly interpret ambiguous
      skill instructions, while a less capable agent may misinterpret and produce
      unintended behavior. The evidence does not test for this variance.
    remediation: |
      Add to EVID-0010.verification.commands:
      - id: CMD-0002
        shell: bash
        command: |
          # Test skill interpretability across capability levels
          apm2 factory skill verify --capability-level novice
          apm2 factory skill verify --capability-level expert
        expected_exit_code: 0
        cwd: .
        timeout_seconds: 600
        network_access: DISALLOWED

  # MODE 36: Assurance-Case / Safety-Case Reasoning
  # Focus: Evaluate evidence sufficiency and claim-argument-evidence structure

  - finding_id: FND-SA3-0004
    source_agent: SA-3
    source_cycle: 1
    source_mode: 36
    category: EVIDENCE_DEFECT
    subcategory: CLAIM_EVIDENCE_GAP
    severity: BLOCKER
    location:
      file: documents/prds/PRD-0005/requirements/REQ-0008.yaml
      section: evidence
    description: |
      REQ-0008 (Determinism envelope and material-diff acceptance) makes a critical
      safety claim: "Re-running compilation with identical inputs MUST produce
      byte-identical artifacts or a bounded diff report requiring explicit acceptance."

      The requirement is linked to EVID-0006 (determinism tests), but the evidence
      contract is insufficient to support the claim. Specifically:

      CLAIM: Byte-identical outputs OR bounded diffs with acceptance
      EVIDENCE: Golden run verification command that checks determinism
      GAP: No evidence artifact demonstrates the "bounded diff classification" logic
      or the "acceptance workflow" for material diffs.

      This is a classic assurance-case failure: beautiful structure, weak evidence.
      The requirement promises diff classification (structural vs free-text) and
      acceptance gates, but EVID-0006 only tests byte-identity.

      An auditor would ask: "What happens when a diff IS detected? Where is the
      evidence that the classification and acceptance logic works correctly?"
    remediation: |
      Split EVID-0006 into two artifacts:

      EVID-0006a: Byte-identical determinism for fixed inputs
      - Verify that identical PRD + repo state + routing profile → identical outputs

      EVID-0006b: Diff classification and acceptance workflow
      - Introduce an intentional non-material change (comment in free-text field)
      - Verify compiler classifies it as "free-text" not "structural"
      - Introduce a structural change (add new requirement)
      - Verify compiler classifies it as "structural" and requires acceptance
      - Verify acceptance workflow (--accept flag, manifest signature)

      Update REQ-0008.evidence.evidence_ids to include both EVID-0006a and EVID-0006b.

  - finding_id: FND-SA3-0005
    source_agent: SA-3
    source_cycle: 1
    source_mode: 36
    category: EVIDENCE_DEFECT
    subcategory: SUBCLAIM_MISSING
    severity: MAJOR
    location:
      file: documents/prds/PRD-0005/requirements/REQ-0009.yaml
      section: acceptance_criteria
    description: |
      REQ-0009 (Auth and provenance integration) makes a security assurance claim:
      "The compiler never relies on ambient credentials for external tool invocations;
      any external invocation must be mediated and recorded."

      Assurance-case decomposition reveals missing subclaims:

      TOP CLAIM: No ambient credential leakage
      SUBCLAIM 1: All external invocations are enumerated and known [MISSING]
      SUBCLAIM 2: Each invocation has explicit credential passing [IMPLIED but not tested]
      SUBCLAIM 3: Invocation log is tamper-evident [MISSING]
      SUBCLAIM 4: No subprocess can inherit environment credentials [MISSING]

      EVID-0008 verifies "security posture checks (redaction, network defaults,
      no ambient creds)" but the evidence commands only run `cargo deny check` and
      a custom security verify command. This does not provide traceable evidence for
      subclaims 1, 3, and 4.
    remediation: |
      Add acceptance criteria to REQ-0009:
      - "The compiler maintains a manifest of all external tools invoked (e.g., cargo
         metadata, git log) with explicit credential scope per invocation."
      - "Subprocess invocations MUST use explicit environment variable passing;
         inheriting parent environment is forbidden."
      - "Run manifest includes cryptographic hash of invocation log; tampering detection
         is mandatory."

      Add new evidence artifact:
      EVID-0008a: External invocation audit and subprocess isolation tests
      - Verify that all external tools are enumerated in a manifest
      - Test that subprocess cannot access parent AWS_ACCESS_KEY_ID or similar
      - Test that invocation log tampering is detected and rejected

  - finding_id: FND-SA3-0006
    source_agent: SA-3
    source_cycle: 1
    source_mode: 36
    category: SPEC_DEFECT
    subcategory: SAFETY_ARGUMENT_INCOMPLETE
    severity: MAJOR
    location:
      file: documents/prds/PRD-0005/08_risks_questions.yaml
      section: risks.RSK-0001
    description: |
      RSK-0001 addresses non-determinism risk, but the mitigation strategy lacks an
      assurance-case structure. The mitigations list techniques (determinism envelope,
      canary mode, staged prompting) but do not form a coherent safety argument.

      A proper safety case would be:

      CLAIM: Compiler outputs are deterministic enough for production use
      ARG-1: Structural outputs are byte-identical (schema-enforced)
        EVID: EVID-0006
      ARG-2: Free-text outputs have bounded variance (classification-gated)
        EVID: [MISSING - see FND-SA3-0004]
      ARG-3: Model swaps require canary validation before acceptance
        EVID: EVID-0007 (partially - canary tests exist)
      ARG-4: Retry policy with tighter constraints prevents runaway variance
        EVID: [MISSING]

      Current mitigation text is a narrative, not a structured argument with evidence
      pointers. This makes it impossible to verify that mitigations are sufficient.
    remediation: |
      Restructure RSK-0001.mitigations as a claim-argument-evidence tree:

      mitigations:
        claim: "Compiler non-determinism is bounded to acceptable levels"
        arguments:
          - argument_id: ARG-RSK-0001-1
            claim: "Structural outputs are byte-identical for fixed inputs"
            evidence_ids: [EVID-0006a]
          - argument_id: ARG-RSK-0001-2
            claim: "Free-text variance is classified and gated"
            evidence_ids: [EVID-0006b]
          - argument_id: ARG-RSK-0001-3
            claim: "Model swaps are canary-validated before adoption"
            evidence_ids: [EVID-0007]
          - argument_id: ARG-RSK-0001-4
            claim: "Retry policy prevents unbounded variance"
            evidence_ids: [EVID-0012-TBD]

  # MODE 46: Multi-Criteria Decision Analysis (MCDA)
  # Focus: Balance competing concerns and make tradeoffs explicit

  - finding_id: FND-SA3-0007
    source_agent: SA-3
    source_cycle: 1
    source_mode: 46
    category: SPEC_DEFECT
    subcategory: TRADEOFF_IMPLICIT
    severity: MAJOR
    location:
      file: documents/prds/PRD-0005/requirements/REQ-0005.yaml
      section: statement
    description: |
      REQ-0005 (RFC and ticket lint extensions) introduces strict lint rules to
      "prevent clutter and accidental duplication." However, the requirement does not
      make explicit the tradeoff between:

      CRITERION 1: Clutter prevention (minimize cousin abstractions)
      CRITERION 2: Developer velocity (minimize lint friction)
      CRITERION 3: Innovation space (allow justified net-new when needed)

      The requirement states lint rules "MUST implement" no-new-module checks and
      cousin abstraction detection, but it does not specify:
      - What threshold triggers a cousin abstraction finding?
      - How strict is "explicit justification" - one sentence or full design doc?
      - Can developers override with a waiver, or is it fail-closed?

      Multi-criteria decision analysis reveals this is a Pareto frontier problem:
      maximizing all three criteria simultaneously is impossible. The PRD must make
      the weighting explicit.
    remediation: |
      Add to REQ-0005 a new section:

      tradeoff_policy:
        criteria:
          - criterion: clutter_prevention
            weight: 0.5
            rationale: "Primary goal per North Star Phase 1 (code quality)"
          - criterion: developer_velocity
            weight: 0.3
            rationale: "Must not block urgent production work"
          - criterion: innovation_space
            weight: 0.2
            rationale: "Allow justified net-new when extension is infeasible"

        decision_rule: |
          Lint rules are fail-closed by default (clutter_prevention priority),
          but developers can request a waiver with:
          - Explicit justification (≥3 alternative extension points considered)
          - Authority signoff from DOMAIN_PRODUCT or DOMAIN_RUNTIME
          - Time-bounded waiver (expires in 90 days, forces cleanup)

      This makes the tradeoff explicit and provides a decision framework for edge cases.

  - finding_id: FND-SA3-0008
    source_agent: SA-3
    source_cycle: 1
    source_mode: 46
    category: SPEC_DEFECT
    subcategory: OPTIMIZATION_UNJUSTIFIED
    severity: MAJOR
    location:
      file: documents/prds/PRD-0005/requirements/REQ-0011.yaml
      section: statement
    description: |
      REQ-0011 (Continuous Refactor Radar) proposes a continuous refactoring stage
      that "emits maintenance recommendations based on CCP signals." However, the
      requirement does not justify the optimization of "continuous" over "on-demand"
      or "periodic" refactoring.

      MCDA criteria for refactoring strategy:

      CRITERION 1: Signal freshness (how quickly we detect duplication)
      CRITERION 2: Noise management (avoid alert fatigue)
      CRITERION 3: Execution capacity (do we have bandwidth to act?)
      CRITERION 4: Predictability (can teams plan around refactor work?)

      The PRD optimizes for CRITERION 1 (continuous = always fresh) but does not
      assess the cost to CRITERION 2 (continuous may create noise) and CRITERION 4
      (unpredictable maintenance tickets disrupt sprint planning).

      Alternative strategies not considered:
      - Weekly batch: Aggregate signals once per week (predictable, lower noise)
      - Threshold-triggered: Emit radar only when hotspot exceeds threshold
      - On-demand: Human operator invokes radar before major refactors

      The requirement states radar is "continuous" but acceptance criteria say
      "deterministic report artifact for a configurable time window (e.g., 7d)" -
      this is actually periodic (weekly), not continuous. The terminology is
      inconsistent with the design.
    remediation: |
      Clarify REQ-0011.statement to specify "periodic" not "continuous":

      "The compiler MUST provide a Refactor Radar stage that periodically (configurable
      interval, default 7 days) emits maintenance recommendations based on CCP signals."

      Add tradeoff_analysis section:

      tradeoff_analysis:
        selected_strategy: periodic_weekly
        alternatives_considered:
          - strategy: continuous
            pros: [always fresh signals]
            cons: [alert fatigue, unpredictable workload]
            rejected_because: "Teams cannot plan around continuous maintenance tickets"
          - strategy: threshold_triggered
            pros: [low noise, actionable]
            cons: [may miss gradual degradation]
            rejected_because: "Complexity of threshold tuning not justified for MVP"
          - strategy: on_demand
            pros: [human control, no surprise work]
            cons: [humans forget to run it, signals go stale]
            rejected_because: "Defeats purpose of proactive maintenance"

  - finding_id: FND-SA3-0009
    source_agent: SA-3
    source_cycle: 1
    source_mode: 46
    category: SPEC_DEFECT
    subcategory: CRITERIA_CONFLICT_UNRESOLVED
    severity: MAJOR
    location:
      file: documents/prds/PRD-0005/05_success_metrics.yaml
      section: metrics.MET-0006
    description: |
      MET-0006 (Compile latency) sets a target of "p50 <= 2 minutes, p95 <= 5 minutes"
      but does not address potential conflict with other success criteria:

      MET-0002 (Requirement mapping completeness): 100% coverage
      MET-0005 (Determinism): >= 95% byte-identical
      MET-0006 (Compile latency): p50 <= 2 minutes

      These criteria may conflict:
      - Achieving 100% mapping completeness may require expensive semantic analysis
        that increases latency
      - Achieving 95% determinism may require multiple retry passes that increase latency

      Multi-criteria decision analysis requires explicit priority ordering when
      criteria conflict. The PRD does not specify: If we must choose between hitting
      the latency target OR achieving 100% mapping completeness, which wins?
    remediation: |
      Add to documents/prds/PRD-0005/05_success_metrics.yaml:

      metric_priority_order:
        rationale: |
          When metrics conflict, this priority order determines which to optimize first.
        order:
          1. MET-0002 (Mapping completeness) - correctness over speed
          2. MET-0005 (Determinism) - reproducibility is essential
          3. MET-0004 (Reuse ratio) - prevents technical debt
          4. MET-0006 (Compile latency) - optimize latency after correctness
          5. MET-0001 (Success rate) - already implied by above
          6. MET-0003 (Ticket noise) - quality over quantity
          7. MET-0007 (Refactor throughput) - continuous improvement

        conflict_resolution_policy: |
          If latency exceeds target while maintaining correctness metrics (1-3),
          this is acceptable for Phase 1. Latency optimization is Phase 2 work.

  # MODE 76: Calibration and Epistemic Humility
  # Focus: Assess confidence levels and uncertainty

  - finding_id: FND-SA3-0010
    source_agent: SA-3
    source_cycle: 1
    source_mode: 76
    category: SPEC_DEFECT
    subcategory: OVERCONFIDENT_CLAIM
    severity: MAJOR
    location:
      file: documents/prds/PRD-0005/03_goals_scope.yaml
      section: goals.G-0001.measurable_outcomes
    description: |
      G-0001 measurable outcome states: "At least 90% of emitted tickets include
      explicit file paths and verification commands." This is a highly confident
      prediction about a novel compiler system's output quality.

      Calibration check: What is the reference class base rate for automated ticket
      generation systems achieving 90% quality on first deployment?

      Without historical data, this target appears overconfident. Typical learning
      curves for LLM-based systems show:
      - First iteration: 40-60% accuracy
      - After tuning: 70-80% accuracy
      - Production-grade: 85-95% accuracy (after months of iteration)

      Setting 90% as a Phase 1 exit criterion may be unrealistic, leading to:
      - Premature declaration of success (lowering evaluation standards)
      - Prolonged Phase 1 with no clear path to Phase 2
      - Gaming the metric (emit fewer tickets to inflate percentage)
    remediation: |
      Revise G-0001.measurable_outcomes to include phased targets with uncertainty:

      measurable_outcomes:
        - outcome: "Ticket quality (explicit paths + verification commands)"
          phase_1_mvp: ">= 70% (confidence: medium)"
          phase_1_production: ">= 85% (confidence: low-medium)"
          phase_2_target: ">= 90% (confidence: medium, after feedback loop)"
          calibration_note: |
            Base rate for automated spec-to-ticket systems is ~60% quality on first
            iteration. We are setting 70% for MVP to allow learning, with stretch
            goal of 85% for production readiness.

      Add retrospective calibration tracking:
      - After 3 months, measure actual ticket quality
      - Compare prediction (70-85%) to outcome
      - Adjust future estimates based on observed accuracy

  - finding_id: FND-SA3-0011
    source_agent: SA-3
    source_cycle: 1
    source_mode: 76
    category: SPEC_DEFECT
    subcategory: UNCERTAINTY_UNQUANTIFIED
    severity: MINOR
    location:
      file: documents/prds/PRD-0005/08_risks_questions.yaml
      section: risks.RSK-0004
    description: |
      RSK-0004 (secret leakage via model routing) assesses likelihood as
      "Low-to-medium depending on defaults" but does not quantify this uncertainty
      or provide a calibrated range.

      Calibration requires numeric estimates:
      - What is the probability of a secret leakage incident in the next 12 months?
      - What is the confidence interval around this estimate?

      "Low-to-medium" could mean:
      - 5-25% (actual low-to-medium risk)
      - 1-10% (low risk with uncertainty)
      - 25-50% (medium risk)

      Without quantification, stakeholders cannot make informed decisions about
      mitigation investment. A 5% risk may not justify heavy mitigation; a 25%
      risk absolutely does.
    remediation: |
      Replace qualitative likelihood with calibrated probability ranges:

      RSK-0004:
        statement: "Model routing profiles leak secrets via prompts/logs or cause
                   sensitive code to be transmitted unexpectedly."
        impact: "High: security incident and loss of trust"
        likelihood_quantified:
          baseline: 0.15  # 15% probability over 12 months
          with_mitigations: 0.03  # 3% probability
          confidence_interval: [0.08, 0.25]  # 80% confidence interval
          calibration_basis: |
            Reference class: LLM integration projects with secret handling
            Historical data: 10-20% experience at least one secret exposure incident
            Our mitigations (default-deny network, redaction) reduce by ~80%
          residual_uncertainty: |
            Main uncertainty: Effectiveness of redaction rules (not yet tested at scale)
            Would increase confidence with: 6 months of production usage data

  - finding_id: FND-SA3-0012
    source_agent: SA-3
    source_cycle: 1
    source_mode: 76
    category: EVIDENCE_DEFECT
    subcategory: CONFIDENCE_LEVEL_MISSING
    severity: MINOR
    location:
      file: documents/prds/PRD-0005/evidence_artifacts/EVID-0001.yaml
      section: verification
    description: |
      EVID-0001 (End-to-end golden run) specifies a verification command with
      expected_exit_code: 0, but does not include confidence level or flakiness
      tolerance.

      Epistemic humility requires acknowledging test uncertainty:
      - What is the probability this test passes when the system is actually correct?
      - What is the probability this test fails when the system is actually broken?
      - Is this test flaky (non-deterministic environment dependencies)?

      Without confidence metadata, we cannot calibrate our trust in evidence. A test
      that passes 95% of the time when correct is very different from one that passes
      60% of the time.
    remediation: |
      Add confidence metadata to all evidence artifacts:

      EVID-0001:
        verification:
          commands:
            - id: CMD-0001
              shell: bash
              command: ./xtask.sh factory-golden --prd PRD-0005 --clean
              expected_exit_code: 0
              confidence:
                sensitivity: 0.95  # P(test fails | system broken)
                specificity: 0.98  # P(test passes | system correct)
                flakiness_rate: 0.02  # 2% false positive rate
                calibration_basis: "Historical test runs on similar golden tests"
              cwd: .
              timeout_seconds: 900
              network_access: DISALLOWED

  # MODE 80: Debiasing / Epistemic Hygiene
  # Focus: Identify cognitive biases and blind spots

  - finding_id: FND-SA3-0013
    source_agent: SA-3
    source_cycle: 1
    source_mode: 80
    category: SPEC_DEFECT
    subcategory: ANCHORING_BIAS
    severity: MAJOR
    location:
      file: documents/prds/PRD-0005/requirements/REQ-0002.yaml
      section: acceptance_criteria
    description: |
      REQ-0002 (CCP generation) includes "churn/hotspot summaries" as part of the
      Codebase Context Pack, but the PRD does not justify WHY churn analysis is
      necessary for requirement-to-component mapping.

      DEBIASING CHECK: This appears to be anchoring bias - the first design included
      churn analysis (possibly borrowed from GitHub's code frequency graphs or similar
      tools), and subsequent iterations never questioned whether it's actually needed.

      DISCONFIRMATION TEST: What would happen if we removed churn analysis?
      - Impact Map quality: Likely unchanged (churn doesn't affect extension point fit)
      - RFC grounding: Unchanged (paths exist regardless of churn)
      - Refactor Radar: This is the only consumer of churn data

      Conclusion: Churn analysis should be in REQ-0011 (Refactor Radar) not REQ-0002
      (CCP generation). Including it in the core CCP adds complexity and coupling
      without clear benefit.
    remediation: |
      Move churn/hotspot analysis from REQ-0002 (CCP) to REQ-0011 (Refactor Radar):

      REQ-0002 (revised):
        statement: |
          The compiler MUST generate a Codebase Context Pack (CCP) for a given
          repository checkout, including:
            (a) a component atlas with stable component IDs and invariants,
            (b) a crate dependency graph derived from cargo metadata,
            (c) a public API entrypoint inventory,
            (d) a prior decisions index.

          [REMOVED: churn/hotspot summaries - moved to REQ-0011]

      REQ-0011 (revised):
        statement: |
          The compiler MUST provide a Refactor Radar stage that generates churn/hotspot
          analysis from repository history and emits maintenance recommendations.

      This reduces REQ-0002 scope and eliminates unnecessary git history dependency
      for core compilation.

  - finding_id: FND-SA3-0014
    source_agent: SA-3
    source_cycle: 1
    source_mode: 80
    category: SPEC_DEFECT
    subcategory: CONFIRMATION_BIAS
    severity: MAJOR
    location:
      file: documents/prds/PRD-0005/03_goals_scope.yaml
      section: goals.G-0004
    description: |
      G-0004 states: "Reduce architectural clutter by defaulting to reuse and making
      duplication risk explicit." The measurable outcomes focus on PREVENTING new
      modules and DETECTING cousin abstractions, but do not measure whether existing
      clutter is actually REDUCED.

      DEBIASING CHECK: Confirmation bias - the PRD assumes that preventing new clutter
      = reducing total clutter, but this is only true if deletion rate exceeds
      creation rate.

      PREMORTEM: "It's 6 months later and the codebase has MORE clutter, not less.
      What happened?"
      - Possible cause: We prevented 50 new modules but only deleted 10 old ones
      - Net result: Clutter reduction rate is negative
      - Root cause: We measured prevention, not reduction

      ALTERNATIVE HYPOTHESIS: The goal should be "Prevent NEW clutter from Phase 1
      work" (achievable) not "Reduce architectural clutter" (requires aggressive
      deletion, which may be out of scope).
    remediation: |
      Either:

      OPTION A: Revise goal to match actual capability
      G-0004 (revised):
        statement: "Prevent new architectural clutter by defaulting to reuse and
                   making duplication risk explicit for Phase 1 work."
        measurable_outcomes:
          - "New modules/crates created in Phase 1 require explicit justification"
          - "Cousin abstraction findings are emitted and tracked"
          - "Net module count increase is <= 10% over 6 months"

      OR

      OPTION B: Add deletion requirements to actually reduce clutter
      Add new requirement:
      REQ-0013: Deletion and consolidation planning
        statement: "For every net-new module introduced, the compiler MUST identify
                   at least one candidate module for deprecation or consolidation."
        measurable_outcome: "Module deletion rate >= 50% of creation rate"

  - finding_id: FND-SA3-0015
    source_agent: SA-3
    source_cycle: 1
    source_mode: 80
    category: SPEC_DEFECT
    subcategory: AVAILABILITY_BIAS
    severity: MINOR
    location:
      file: documents/prds/PRD-0005/requirements/REQ-0007.yaml
      section: acceptance_criteria
    description: |
      REQ-0007 (Multi-model routing) includes "Canary mode can run two routes for a
      stage and compare structured outputs" as an acceptance criterion, suggesting
      canary testing is a high-priority feature.

      DEBIASING CHECK: Availability bias - this feature may be overweighted because
      canary testing is a vivid, recent pattern from other systems (Kubernetes,
      feature flags, etc.).

      BASE RATE CHECK: What percentage of compiler stages will actually benefit from
      canary testing?
      - CCP generation: Deterministic, no model involved → canary not applicable
      - Impact Map: Model-driven but schema-constrained → low variance
      - RFC framing: High variance, canary valuable here
      - Ticket decomposition: Medium variance, canary possibly valuable

      Estimate: 2 out of 6 stages benefit from canary. Building canary infrastructure
      for 33% utilization may not be justified for Phase 1 MVP.

      ALTERNATIVE: Defer canary to Phase 2, use manual A/B comparison for Phase 1.
    remediation: |
      Move canary mode from REQ-0007 acceptance criteria to "out_of_scope" for Phase 1:

      scope:
        in_scope:
          - "Model routing with explicit profiles and provenance"
          - "Manual comparison of two routing profiles (diff reports)"
        out_of_scope:
          - "Automated canary testing (deferred to Phase 2 after baseline established)"

      Add to open_questions:
      Q-0004:
        question: "Which compiler stages have sufficient output variance to justify
                  automated canary testing, and should this be Phase 1 or Phase 2 work?"
        decision_needed_by: "Before implementing canary infrastructure"
        resolution_plan: "Gather 3 months of routing variance data, then decide"

  - finding_id: FND-SA3-0016
    source_agent: SA-3
    source_cycle: 1
    source_mode: 80
    category: SPEC_DEFECT
    subcategory: PLANNING_FALLACY
    severity: MAJOR
    location:
      file: documents/prds/PRD-0005/05_success_metrics.yaml
      section: metrics.MET-0006
    description: |
      MET-0006 (Compile latency) targets "p50 <= 2 minutes, p95 <= 5 minutes" for
      a complex multi-stage compiler pipeline that includes:
      - CCP generation (cargo metadata, git analysis, YAML parsing)
      - Impact Map generation (semantic analysis, component matching)
      - RFC framing (LLM calls, template rendering)
      - Ticket decomposition (LLM calls, validation)

      DEBIASING CHECK: Planning fallacy - underestimating task duration by focusing
      on best-case scenario and ignoring historical base rates.

      REFERENCE CLASS FORECASTING: What is the typical latency for comparable systems?
      - Cargo metadata: 5-30 seconds (depends on workspace size)
      - LLM inference: 10-60 seconds per stage (depends on context size, model)
      - YAML parsing + validation: 1-5 seconds
      - Rough estimate: 4 stages × 30s avg = 120s = 2 minutes (optimistic)

      This matches the p50 target, but p50 assumes EVERYTHING goes right:
      - No retries due to schema violations
      - No LLM timeouts or rate limits
      - No cache misses
      - No network hiccups

      Historical data on multi-stage pipelines: Actual latency is typically 2-3x
      initial estimate.
    remediation: |
      Apply reference-class adjustment to latency targets:

      MET-0006 (revised):
        name: Compile latency
        target: "p50 <= 4 minutes, p95 <= 10 minutes (Phase 1 MVP)"
        stretch_goal: "p50 <= 2 minutes, p95 <= 5 minutes (Phase 2 after optimization)"
        calibration:
          reference_class: "Multi-stage LLM pipelines with validation"
          base_rate_latency: "p50 = 3-5 min, p95 = 8-12 min"
          adjustment_rationale: |
            Setting 4 min / 10 min targets accounts for:
            - LLM inference variability (1-2 min per stage)
            - Retry policy overhead (20-30% time increase)
            - Cache cold-start scenarios (first run penalty)

          optimization_roadmap:
            - phase_1: Establish baseline, no premature optimization
            - phase_2: Parallel stage execution, aggressive caching
            - phase_3: Fine-tuned models (smaller, faster)

north_star_assessment:
  primary_phase: 1
  phase_scores:
    phase_1:
      score: 0.85
      direct_contribution: 0.9
      enabling_contribution: 0.8
      no_harm: 0.85
      rationale: |
        PRD-0005 directly advances Phase 1 (Recursive Self-Improvement) by improving
        agent output quality through deterministic spec compilation. The Idea Compiler
        prevents cousin abstractions, grounds RFCs in repository truth, and provides
        low-noise tickets - all of which directly support the exit criterion
        "agent-authored code passes review with ≤50% findings vs human baseline."

        Direct contribution (0.9): High. This is core Phase 1 infrastructure.
        Enabling contribution (0.8): High. CCP and Impact Map are reusable for Phase 2+.
        No harm (0.85): Minor concern - complexity may slow initial velocity (see findings).

    phase_2:
      score: 0.65
      direct_contribution: 0.3
      enabling_contribution: 0.9
      no_harm: 1.0
      rationale: |
        PRD-0005 indirectly enables Phase 2 (Novel Methods Generation) by establishing
        a deterministic knowledge capture system. The CCP's prior decisions index and
        component atlas create a foundation for identifying novelty (semantic distance
        from existing abstractions).

        Direct contribution (0.3): Low. No explicit novelty detection in Phase 1 scope.
        Enabling contribution (0.9): High. CCP is prerequisite for novelty assessment.
        No harm (1.0): No obstacles to Phase 2 work.

    phase_3:
      score: 0.45
      direct_contribution: 0.1
      enabling_contribution: 0.7
      no_harm: 1.0
      rationale: |
        PRD-0005 has minimal direct impact on Phase 3 (Company Formation) but enables
        IP tracking through provenance manifests and decision history.

        Direct contribution (0.1): Very low. Company formation is far downstream.
        Enabling contribution (0.7): Medium. Provenance system supports patent filing.
        No harm (1.0): No obstacles.

    phase_4:
      score: 0.35
      direct_contribution: 0.0
      enabling_contribution: 0.6
      no_harm: 0.95
      rationale: |
        PRD-0005 has no direct impact on Phase 4 (Corporate Partnerships) but the
        deterministic compilation system could be a partnership offering (license the
        compiler to partners for their own agent systems).

        Direct contribution (0.0): None. Partnerships are far future.
        Enabling contribution (0.6): Medium. Compiler could be a product.
        No harm (0.95): Minor concern - proprietary compiler may complicate partnerships
                       if partners want to contribute but can't access internals.

    phase_5:
      score: 0.25
      direct_contribution: 0.0
      enabling_contribution: 0.5
      no_harm: 1.0
      rationale: |
        PRD-0005 has no direct impact on Phase 5 (Planetary Impact / Life Sciences)
        but the recursive improvement system (Phase 1) is a prerequisite for everything
        downstream, including Phase 5.

        Direct contribution (0.0): None. Life sciences work is terminal phase.
        Enabling contribution (0.5): Medium-low. Many steps between this and Phase 5.
        No harm (1.0): No obstacles.

strategic_recommendations:
  phase_acceleration:
    - recommendation: |
        To accelerate Phase 1 exit: Prioritize ticket quality (MET-0003) over latency
        optimization (MET-0006). Quality directly impacts "agent code passes review"
        criterion; latency does not.
    - recommendation: |
        Add explicit "review findings reduction" metric to connect PRD-0005 outputs
        to Phase 1 exit criteria. Track: "Percentage of agent PRs passing review on
        first submission" before and after Idea Compiler deployment.

  cross_phase_synergy:
    - recommendation: |
        Design CCP to be forward-compatible with Phase 2 novelty detection. Add a
        "semantic fingerprint" field to component atlas entries that can later be
        used for novelty distance calculation.
    - recommendation: |
        Include provenance metadata in run manifests that supports patent filing
        (Phase 3). Track: original author, timestamp, prior art references.

  risk_mitigation:
    - recommendation: |
        Mitigate complexity risk (RSK-0003) that could slow Phase 1: Implement
        maturity-based lint enforcement (warning-first, then fail-closed) to avoid
        early adoption friction.
    - recommendation: |
        Mitigate secret leakage risk (RSK-0004) that could jeopardize future phases:
        Require security audit before any multi-tenant deployment (relevant for
        Phase 4 partnerships).

summary:
  total_findings: 16
  by_severity:
    BLOCKER: 1
    MAJOR: 10
    MINOR: 5
    INFO: 0

  by_category:
    SPEC_DEFECT: 11
    EVIDENCE_DEFECT: 5
    TRACEABILITY_DEFECT: 0
    FORMAT_DEFECT: 0

  key_themes:
    - theme: Evidence Sufficiency
      description: |
        Multiple BLOCKER/MAJOR findings related to weak evidence for strong claims.
        PRD makes ambitious safety and determinism claims but evidence artifacts do
        not fully support them (assurance-case gaps).

    - theme: Implicit Tradeoffs
      description: |
        Several requirements optimize for one criterion without acknowledging costs
        to other criteria (lint strictness vs velocity, continuous radar vs
        predictability). Multi-criteria decision analysis reveals unresolved conflicts.

    - theme: Overconfidence
      description: |
        Success metrics (90% ticket quality, 2-minute latency) appear overconfident
        given reference class base rates for similar systems. Calibration and planning
        fallacy corrections suggest more conservative targets for Phase 1.

    - theme: Cognitive Biases
      description: |
        Debiasing analysis reveals anchoring (churn in CCP), confirmation (clutter
        reduction claims), and availability (canary over-prioritization) biases that
        may be inflating scope unnecessarily.

  overall_assessment: |
    PRD-0005 is strategically sound and strongly aligned with Phase 1 objectives
    (score 0.85). However, it contains significant evidence gaps (BLOCKER: FND-SA3-0004)
    and multiple MAJOR issues related to implicit tradeoffs, overconfident predictions,
    and cognitive biases.

    From a stakeholder perspective, the PRD correctly models the three customer
    segments (human operator, compiler holon, ticket executors) but underspecifies
    failure recovery communication and confidence signaling (FND-SA3-0001, FND-SA3-0002).

    From an assurance-case perspective, the PRD has beautiful structure but weak
    evidence in critical areas (determinism, security). Strengthening evidence
    artifacts is essential before production deployment.

    From a multi-criteria decision analysis perspective, the PRD optimizes too
    aggressively for single criteria without making tradeoffs explicit. This will
    cause friction during implementation when teams face conflicting requirements.

    From a calibration perspective, success metrics are likely overconfident by
    20-30% based on reference class forecasting. Adjusting targets to reflect
    historical base rates will prevent premature success declarations.

    From a debiasing perspective, several design choices appear driven by cognitive
    biases rather than first-principles reasoning. Removing anchored features (churn
    in CCP) and deferring availability-biased features (canary testing) would reduce
    scope without harming core goals.

verdict: NEEDS_REVISION
verdict_reason: |
  One BLOCKER-severity finding (FND-SA3-0004: claim-evidence gap for determinism)
  and ten MAJOR-severity findings across stakeholder modeling, assurance case,
  multi-criteria decision analysis, calibration, and debiasing.

  PRD is strategically sound but requires evidence strengthening and tradeoff
  clarification before approval.
